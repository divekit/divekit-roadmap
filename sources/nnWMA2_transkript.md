## Transkript des Interviews mit n.n. WMA 2

**I [00:00:00]** Recording in progress. Ja dann, Hallo zum Interview. Ja, wir nehmen das Ganze auf und würden das Ganze nachher als Transkript dann gerne hochladen. Jetzt vielleicht auch nochmal. Du hast ja schon schriftlich eingewilligt. Aber nochmals manche Einwilligungen, dass wir es ja auch haben. Ist das okay für dich? Wenn wir das dann nachher bei github hochladen?

**B [00:00:24]** Ja, ist in Ordnung.

**I [00:00:28]** Okay. Ja, dann können wir direkt anfangen mit den Fragen erst einmal generell zu dir. Du bist ja wissenschaftlicher Mitarbeiter. Welche Aufgaben genau hast du an der TH, also an welchen Modulen, bist du beteiligt und was ist deine Verantwortung für die einzelnen Module, in denen du tätig bist?

**B [00:00:53]** Also Aufgaben erst einmal generell Teil Lehre, Teil Software-Entwicklung. Der Teil der Softwareentwicklung ist denke ich mal nicht relevant. Also ich entwickle halt einfach Tools, interne Tools, die wir an der Hochschule benutzen. Das mache ich aber auch schon eigentlich seit dem ich da arbeite. Genau. Und Lehre hat sich bei mir relativ stark gewandelt. Ich hab am Anfang halt primär Abnahmen und sowas alles gemacht. Und dann hab ich irgendwann mal angefangen selber die Praktika zu machen. Mittlerweile habe ich sogar teilweise ganze Lehrveranstaltungen übernommen, mit Prüfung und allem. In erster Linie biete ich ein eigenes WPF an, da geht es um iOS Entwicklung für die Bachelor-Studiengänge. Da ist aber eigentlich dieser Normale. Also da gibt es keine Vorleistung, die man bringen soll. Also da gibt es quasi kein Praktikum bei, da geht es nur quasi um Vorlesung und Prüfung. Dann hab ich Paradigmen der Programmierung übernommen, mit Vorlesung und Praktika und Klausur. Da mache ich eigentlich alles. Also da hab ich komplette Freiheit der Lehre und darf alles selber entscheiden und auch alles selber machen. Und dann mach ich noch AP2. In AP2 mache ich eigentlich größtenteils nur das Praktikum und sorge halt dafür, dass möglichst viele Studis da ihr Testat bekommen. Ich bin noch ein bisschen in anderen Modulen drinne, aber die sind glaub ich auch nicht relevant für das Tool hier. Also ich bin noch im Beautiful Code Master, da halte ich drei Vorlesungen und ich bin noch in Compilerbau und Programmiersprachen, wobei ich da gar nicht mehr aktiv bin. Da bin ich quasi nur so am Anfang dabei bis es läuft, dass das Modul zustande kommt und im ersten Semester war ich dabei. Mittlerweile bin ich eigentlich höchstens als Gast da, als Zuhörer selbst.

**I [00:02:56]** Okay, ja, wie gesagt übernimmst du in verschiedenen Modulen Aufgaben der Lehre. Welche Tools und so weiter setzt du dafür ein? Also, welche Tools und welche Medien?

**B [00:03:14]** Also in beiden, also ich rede jetzt primär von Paradigmen und AP2, weil ich glaube, da ist das Tool am relevantesten. In Paradigmen setze ich als Medien eigentlich Screen Casts ein größtenteils, also Videos, die ich selber aufgezeichnet hab, die alle auf YouTube sind. Und Live Coding mit Zoom und so'n Kram. Na ja, sowas ist auch mit dabei. Und Übungen. Also Übungsblätter, sprich PDFs werden ausgeteilt mit Übungsaufgaben, die man nachcoden kann. Und das war's an Medien. In AP2 ist es genau das gleiche, weil wir PP mittlerweile ähnlich machen wie AP2, also nicht ganz so, aber ähnlich. Da kommen halt auch haufenweise Screen Casts zum Einsatz. Eine Vorlesung, wo kein Frontalunterricht mehr stattfindet, sondern eher interaktives Coden und quasi eher um Themen zu festigen als neue Themen beizubringen. Weil das Beibringen wird halt über die Vorlesung über die Screeen Casts abgehandelt. Und im Praktikum auch quasi Übungsblätter die zum Üben dienen und dann quasi ein Ilias Test den jeder Student machen muss. Davon gibt es quasi pro Aufgabe ein Stück. Die haben unterschiedlich viele Chancen den Test zu bestehen. Also die haben mehrere Möglichkeiten, den Test wahrzunehmen. Und ja, das findet im E-Assessment statt, sodass es quasi mit der Prüfungs Plattform der TH Köln abgebildet.

**I [00:04:46]** Gibt es große Unterschiede zu den anderen Modulen, von denen du vorhin auch geredet hattest? Oder sind die generell für DiveKit nicht relevant?

**B [00:04:58]** Die sind wahrscheinlich nicht relevant. Also ich kenne das Tool ehrlich gesagt nicht so gut. In dem WPF was ich mache, da gibt es eigentlich nur Vorlesung, Screen Cast und ein Projekt was die Studis machen. Da haben wir nix mit Zwischenleistungen, die abgenommen werden müssen. In dem anderen WPF mit dem Compiler Bau ist es genauso. Das ist ja in WPFs eigentlich üblich. Und in Beautiful Code da ist auch alles Projektarbeit. Also da gibt es eigentlich keinerlei Dinge, keinerlei Testate die Studenten halt sammeln müssen. Das entfällt dort komplett. Also es bezieht sich wirklich nur auf AP2 und Paradigmen und da ist halt dieses Vorgehen mittlerweile gleich, dass die halt alle einen Test machen müssen.

**I [00:05:45]** Ja okay, gut. Jetzt während Corona ist ja generell alles digital. Was würdest du denn sagen sind denn die größten Herausforderungen? Wenn du jetzt die ganzen Module digital abhalten musst und was wünschst du dir, das man verbessern könnte, um digital besser lehren zu können?

**B [00:06:17]** Also digital besser lehren... Also was mir da jetzt mal grundsätzlich fehlt ist die Deutung von Mimik und Gestik der Studierenden. Was den Vorlesungsteil angeht, den hat man einfach nicht, wenn man Screen Casts up Front bereitstellt. Das habe ich aber kompensiert oder versuche es zu kompensieren mit einem wöchentlichen Termin, der halt so als Vorlesungsersatz dient, wo man einen ein Zoommeeting hat, wo die Leute halt Fragen stellen können. Das heißt, dort hat man wenigstens quasi diesen Kanal von Studi zu Dozent. Das hat man etabliert. Das Problem ist aber trotzdem, dass man halt nicht anhand von Reaktionen, also nonverbalen Reaktionen sehen kann, ob das, was halt passiert, richtig ist. Natürlich fordere ich die Studis auf, gerne die Kamera anzumachen, aber bei mehr als 100 Leuten macht das keiner. In kleinen Grüppchen hast du halt so, weiß ich nicht. In meinem WPF sind ja so 30 Leute, da hast du so 5 bis 10 Leute, die auch die Kamera anmachen, weil es eine kleinere Runde ist, weil es fortgeschrittener im Semester ist. Aber in AP2 beispielsweise oder im PP kommt das eigentlich kaum vor. Du brauchst halt einfach so eine Welle von Studenten, die mitmachen, vereinzelt machen die Leute das nicht.

**B [00:07:42]** ALso da müssen schon mehrere greifen. Das ist auf jeden Fall ein großer Nachteil was die Vorlesung angeht. Was das Praktikum angeht, sind Feedback-Kanäle. Finde ich auch schwierig. Und zwar haben wir durch unser Modell, was wir im Praktikum machen, ein sehr großes Kann-Angebot geschaffen. Das heißt, die Studenten können Beratungstermin wahrnehmen. Die Studenten können quasi Feedback von uns bekommen, müssen sie aber nicht und sie können ohne alldem das Praktikum bestehen. Theoretisch. Und ich hätte es aber gerne eigentlich eher verpflichtend, dass die Leute Feedback bekommen zu ihren Aufgaben. Weil die Studenten müssen nämlich auch nur 50 Prozent des Tests bestehen und wenn ein Student 50 Prozent hat, dann ist es meistens nicht gut und damit fallen die spätestens in der Klausur auf die Nase. Und es wäre gut quasi alle Leute, die halt gerade so, also so die 50 bis 70 Prozent Leute, dass man denen quasi Feedback geben kann für die Art und Weise, wie sie die Lösungen gemacht haben. Viele sagen "Okay, ich konnte das einfach nicht", aber viele sagen auch einfach "Ja, ich hab mein Bestes getan. Ich weiß es aber nicht besser. Sag mir mal was". Und diese Kanäle, die fehlen halt. Die hat man halt im Präsenztermin. Da kann man sowas machen. So ist es halt relativ schwierig.

**I [00:09:11]** Also generell wenn du Aufgaben stellt, dass es da quasi verpflichtend ist, das Feedback zu bekommen. "Ja, die Aufgabe war zu schwer" oder "Ja, war kein Problem".

**B [00:09:20]** Nicht nur das, genau. Also erst einmal das und zweitens auch.. Also ob das zu schwer ist, das sehe ich meistens an der Statistik. Aber ich bekomme vereinzelt Mails von Studenten, die sagen, "ich bin selber enttäuscht von mir, dass ich so schlecht war. Was kann ich denn besser machen?" Und das muss ich dann halt individuell abfrühstücken. Aber besser wäre es, wenn man diesen Leuten halt einen Raum gibt, also einen physischen Raum gibt, wo sich diese Leute treffen können und wirklich Hilfe bekommen, aktive Hilfe bekommen. Und das Problem ist halt ich glaube, wenn man zu viel Kann-Angebote hat und zu viel Remote hat, hat man einfach diese diese Hürde nicht mehr. Wenn man eh am Campus ist, dann nimmt man eher die Chance wahr, so ein Angebot wahrzunehmen, als wenn man eh nur zu Hause sitzt. Weil dann hab ich so das Gefühl, dass das Studieren eh so nebenbei passiert. Also man hat halt viel Flexibilität, aber die Wenigstens sind halt diszipliniert und das ist etwas, was ich halt persönlich merke. Das merke ich an der Leistung der Studierenden. Und was halt auch ein bisschen schlecht ist an dieser ganzen online Geschichte ist halt einfach die extrem hohe Cheating-Rate. Also wenn man den Leuten die Möglichkeit gibt. Also die müssen ja bei uns einen Test machen, der am Freitag immer stattfindet und sie haben viermal die Möglichkeit im Semester diesen Test zu machen oder dreimal weiß nicht mehr. Und die WhatsApp-Gruppen der Studierenden sind voll mit Lösungen. Also während des Tests werden die ganze Zeit Lösungen ausgetauscht. Und das ist halt für uns kacke, weil wir geben uns Mühe. Wir geben uns Mühe, an der Variation was zu tun und wir geben den Studenten schon so viele Möglichkeiten. Aber das wird halt irgendwo exploited. Und wenn man jetzt das Ganze vor Ort machen würde, dann würde man ja quasi die Leute live strugglen sehen, wenn tatsächlich halt irgendwie gecheated wird oder wenn man gecarried wird. Und so auf dem auf dem kompletten Online Weg kriegste das halt nicht nachverfolgt. Also es gibt natürlich so Situationen, wo du das wirklich erkennst. Also wurde so semi automatisiert Detection hast, dass da einfach das Gleiche steht und es keinen Sinn macht, es das gleiche ist. Es ist aber alles mit sehr viel Aufwand unsererseits verbunden und die meisten sagen dann "ja, kommen ist egal." Also dann ist es halt so, weil spätestens in der Klausur fallen die halt alle auf die Füße und kriegen es nicht hin. Und ja also ich denke mal, das ist etwas, was zum Beispiel vor Ort besser funktioniert. Dieses Modell, das wir aktuell haben, das funktioniert hat gerade online schlecht. Also diese ganze Cheating Präventions Geschichten.

**I [00:12:08]** Also wenn ich das richtig rausgehört habe. Die Aufgaben, die gestellt werden, die sind teilweise individualisiert. Also sowas ist generell gewünscht, auch mit dem Tool dann, dass sich die Aufgaben gut automatisieren lassen.

**B [00:12:25]** Genau. Individualisieren und automatisieren. Automatisieren und individualisieren sind so eine Sache. Also über das E-Learning Ding, ich weiß nicht, ob ihr damit Erfahrung habt. Ich weiß nicht, ob ihr mal E-Klausuren gemacht habt, aber dort ist das so je nachdem was für eine Frage du stellst, kannst du halt die automatisiert auswerten lassen. Also ganz einfaches Beispiel: Du sollst z.B. Begriffe ordnen in einer Reihenfolge oder du sollst z.B. Lückentext Dinge befüllen. Oder du sollst eine Antwort auf eine Frage geben, die z.B. eine Zahl ist. Das alles kannst du automatisiert auswerten, das ist kein Problem und du musst aber selber dafür Sorge tragen, dass du eine aktive Variation reinbringt. Also du musst quasi die Frage duplizieren und eine Variation reinbringen und auch dafür sorgen, dass diese Variation ungefähr gleich schwer ist und gleich viele Punkte gibt. Weil ansonsten ist das halt kacke für die Studis. Das ist etwas, was man aktiv tun soll. Das Problem ist aber diese ganzen Dinger sind hochgradig kopierbar. Du kannst also einmal einen Screenshot machen von diesen Sachen und dann hast du halt schon mal 30 Prozent des Tests für immer, so nach dem Motto. Das heißt, die spannenderen Sachen sind die ganzen Freitextaufgaben. Wo zum Beispiel auch die Coding-Aufgaben drunter fallen. Die fallen ja alle unter Freitext, weil die Leute müssen ja Coden und Coding-Aufgaben kannst du auch automatisiert auswerten. Das ist aber sehr schwierig und das ist vielleicht auch nicht so fair für die Studenten, finde ich. Weil wenn du selber da drüber guckst, dann siehst du ja teilweise auch Intentionen der Studenten und dafür kannst du wenigstens Teilpunkte geben. Dann gibt es halt die Hardliner, die sagen entweder ganz oder gar nicht oder es gibt so Leute, die halt gucken. Okay, also das kompiliert zwar nicht, aber es ist vom Sinn her richtig. Vom Gedanken her ist es richtig. Deswegen gibt es halt wenigstens Teilpunkte dafür. Deswegen machen wir das alles als Freitext. Also wir machen Hälfte automatisiert, Hälfte Freitext immer. Und ja, bei den automatisierten Dingern hast du natürlich immer die höchste Success Rate. Das sehen wir auch in der Statistik. Da sind die quasi die volle Punktzahl. So auf 60 bis 70 Prozent. Und bei den ganzen Freitextaufgaben ist man in der Regel durchschnittlich bei 50 bis 60 Prozent Success Rate. Und ja, das ist aber die einzige Möglichkeit, die wir gerade haben. Und wie gesagt, auch bei der Freitextaufgabe muss man aktiv Variation reinbringen. Also Klassen umbenennen, andere semantische Beispiele bringen, wenn man über das Level von "ich benenne nur Variablen um" hingehen möchte und wirklich eine andere Aufgabe stellt die aber logisch das Gleiche ist, muss man viel aktiv denken.

**I [00:14:46]** Nochmal so generell, also die Aufgaben die in PP und in AP2 gestellt werden, die beziehen sich sehr viel auf Coding, das hab ich mitgenommen. Und Lückentext und generell Wissensfragen sollten eher Textaufgaben sein und nichts mit Zeichnungen oder sonstiges.

**B [00:15:09]** Tatsächlich ist das halb halb. Also also nicht ganz halb. Also ich würde sagen 60 bis 70 Prozent Coding und 30 bis 40 Prozent Theorie. Theorie besteht so aus Single Choice, Multiple Choice. Zeichnungen nicht, aber ich habe z.B. ein Diagramm oder irgendein Klassen Diagramm, keine Ahnung was, und da hab ich so Lücken drinnen und die sollen die Lücken dann zuordnen. Sowas mach ich mal, wenn es sich anbietet. Also es gibt auch so ein bisschen kreative Aufgaben, aber ich falle dann auch manchmal zurück auf so simple Aufgaben. Also so simpel im Sinne von jetzt nicht kreativ, aber dafür halt trotzdem didaktisch halt sinnvoll. Das ist aber in der Balance auf jeden Fall mehr Coding.

**I [00:15:58]** Und du stellst die Aufgaben selber auch generell. Ja und du kontrollierst die Aufgaben auch alleine?

**B [00:16:07]** In PP hab ich das gemacht. Das war zu viel. Mittlerweile haben wir im AP2 Team so ein Drittel der Leute, die helfen mir bei den Korrekturen der Tests. Dann macht man das einfach so, dass jede Variation, die es gibt, beantwortet dann eine Person. Und wenn es drei Aufgaben gibt, mit vier Variationen, bekommt halt jeder eine. Also dann muss jeder drei Aufgaben bearbeiten oder vier, je nachdem, wie man es dreht. Und wir sind fünf Leute, die halt die Tests auswerten. Und dann haben wir halt quasi in der Regel 15 Variationen, die wir machen können, über den ganzen Test verteilt. Und dann macht jeder halt halt drei. Das ist eigentlich ganz gut, weil dann ist man in einer Stunde durch. Da haben wir das einfach im wahrsten Sinne des Wortes parallelisiert, was halt wirklich besser ist. Also alleine ist das halt echt Pain, weil da muss man einen Tag lang die Scheiße machen und das ist halt einfach nur anstrengend. Und irgendwann mal siehst du das auch nicht mehr. Also irgendwann mal vergibst du dann wirklich aufgrund deiner physischen Müdigkeit und psychischen Müdigkeit einfach keine Punkte oder nicht die richtigen Punkte, die du eigentlich hätte es vergeben müssen. Also das ist auch zugunsten der Studenten.

**I [00:17:21]** Du hast ja vorhin gesagt, das ist teilweise schon automatisiert, aber wie könnte ein Tool wie DiveKit das Ganze noch verbessern? Also die Aufgaben zu stellen und auszuwerten?

**B [00:17:38]** Ja, also ich kenne dieses Tool nicht, aber ich kann mal sagen, was ich will, was ich gerne als als Möglichkeit hätte. Und zwar je nachdem, welche Art von Aufgaben man hat, bieten sich solche automatisierten Unit Test Aufgaben an. Das habe ich auch gemerkt. Es geht nicht für alle Aufgaben. Es geht aber für viele Aufgaben, wo man quasi sagt okay, ihr bekommt alle ein Projekt Template. Dieses Projekt Template hat halt Unit Tests drinnen und ihr habt halt quasi das bestanden, wenn alle Tests grün laufen und wenn man es halt richtig macht und je nachdem was für eine Aufgabe das ist, gibt es quasi fast nur eine richtige Implementierung um sie grün zu bekommen. Und dann ist es auch egal in welcher Art und Weise die Leute coden. Es ist nur wichtig, dass sie quasi dann diesen Contract des Unit Tests erfüllen. Und das kann man halt sehr schön automatisieren mit Git Repos und mit irgendwelchen automatisierten Pipelines und sowas alles. Das funktioniert. Das hab ich mal ausprobiert. Also alleine mit GitHub Actions und so einem Kram habe ich das mal ausprobiert. Das geht. War aber zu kurzfristig für das Semester. Aber das war so 2 Wochen vorher. Sowas kann ich mir gut vorstellen, aber ich glaube nicht als Ersatz. Weil mir fehlt dann dieser ganze theoretische Unterbau. Also ich möchte auch Theoriefragen abtesten. Aber zumindest so einige Code-Geschichten, weil Code ist immer manuelles Review. Und wenn man dann diese Code-Geschichten halt irgendwie automatisiert reviewen könnte, dann würde man auf jeden Fall schon mal Zeit sparen. Und was man dann ja machen kann ist man sieht ja z.B. auch, wie viele Tests gefailed sind für die jeweilige Person. Und dann könnte man vielleicht auch mit so einer Heuristik sagen Okay, wenn da jetzt irgendwie 80 Prozent der Tests gefailed sind oder 70 oder keine Ahnung was, dann pickt man die Leute raus und gibt denen halt noch mal Feedback, wie sie es besser machen können oder irgendwie sowas. Das kann man dann ja auch quasi detektieren, muss man dann auch gucken, wie gut das funktioniert. Aber das wäre so eine Möglichkeit, die ich mir vorstellen könnte.

**I [00:19:44]** Also die normalen Aufgaben, die man gut automatisieren kann einfach so lassen. Die theoretischen und die Coding-Aufgaben dann mit DiveKit oder so umsetzen.

**B [00:19:57]** Ich weiß nicht, wie gut man Wissensfragen automatisieren kann die Freitextaufgaben sind, da hab ich noch keine Erfahrung. Vielleicht gibt es da auch Tools oder Modelle wie man das macht. Je nachdem was für eine Frage man halt hat, ist da sehr viel Interpretationsspielraum. Also die Formulierung ist nie eindeutig genug, dass man sagen kann "Okay, das gibt Punkte" oder nicht. Das kann man im E-Assessment auch machen. Also du kannst Punkte vergeben nach richtigen Begriffen in einem Freitext. Aber wenn die Begriffe einfach so random auftauchen und du den Satz nicht siehst, der den ganzen Kontext gibt. Ist es auch schwierig. Deswegen hab ich bis jetzt immer die Finger davon gelassen.

**I [00:20:42]** Gibt es denn Aufgaben, die du jetzt momentan nicht stellen kannst, weil die Tools, die du bislang verwendest, das nicht hergeben? Also gibt es irgendwelche Aufgaben, die du überlegt hast, die wären gut für AP2 oder Paradigmen. Welche Möglichkeiten würden dir dafür fehlen?

**B [00:21:05]** Ja, also was ich mir vorstellen kann sind Aufgaben, die ein komplexeres Setup benötigen. Also meine ganzen Coding Aufgaben sind sehr simpel und die benötigen keinerlei Bibliotheken oder Frameworks oder Dependencies. Einfach nur, damit die Studis quasi aus der Aufgabe den Code kopieren können, bei sich einfügen können und coden können. Ich kann mir halt vorstellen, gerade Richtung PP, wenn man beispielsweise ein IntelliJ-Projekt ausliefert direkt mit einem Gradle-File das die Dependencies drin hat, weil man z.B. was mit Coroutines machen will und Coroutines ist nicht in der Standardbibliothek von Kotlin. Das ist ja auch eine Dependency. Diese Aufgaben hab ich nie gestellt, weil ich den Studenten in einer Stunde Prüfungssituation nicht abverlangen wollte ein Download zu machen, das in INtelliJ zu importieren und Gradle seine Arbeit machen zu lassen, weil Gradle für mich nicht stabil genug funktioniert. Und das liegt am Setup. Das liegt nicht an Gradle. Es liegt eigentlich eher am Setup von IntelliJ. Das ist so eine Aufgabenart, die ich nie gemacht habe. Das könnte man lösen, wenn es eine online IDE gäbe, die direkt einen festen Dependency Kram drin hätte. Was man fairerweise sagen muss, man muss ja nicht Gradle benutzen. Man kann ja auch einfach eine jar-Datei mit shippen in dem IntelliJ Projekt die hard drin ist. Da hat man auch keine Versionsprobleme und alles, das geht auch. Aber wie gesagt, ich wollte halt die Leuten nicht dazu zwingen quasi ein IntelliJ Projekt importieren zu lassen, wo es zu Fehlern kommen kann. Das ist eine Art von Aufgabe, die ich nie gestellt habe. Und auch generell, also einfach nur, dass ich selber diesen Zeitconstrain von einer Stunde gebe. Also ein Test dauert genau eine Stunde und da hab ich in der Regel so 6 Fragen. Wie gesagt Hälfte automatisiert, Hälfte Coding. Das heißt die Coding-Aufgabe nimmt niemals mehr als eine halbe Stunde Zeit ein und zwar eine halbe Stunde auf Studentenzeit gerechnet. Also ich guck immer, dass ich die Aufgabe von null auf hundert in einem Drittel der Zeit wie die Studis lösen kann. Wenn das ist, dann ist die Aufgabe genug komplex, finde ich. Und ich kann halt keine großen komplizierten Aufgaben stellen, z.B. auch Aufgaben, die schon eine vorgegebene Klasse haben. Manchmal mach ich das. Also meistens müssen die einfach so selber alles schreiben. Manchmal gebe ich aber auch einfach so ein Stück vor und die müssen irgendwas da drin implementieren. Und die Studis, die fangen halt an alles sehr genau zu lesen und alles sehr genau zu explorieren und verlieren damit sehr viel Zeit. Jetzt kann ich natürlich sagen, das ist deren Problem, aber ich mache mir dann extra die Arbeit und mach solche Aufgaben nicht, obwohl ich die gut finden würde. Also ganz einfaches Beispiel: Ich habe keine einzige Aufgabe, wo ich mehr als zwei Klassen drin habe und sogar mehr als zwei Dateien. Also ich hab immer nur alles in einer Datei, damit die quasi nicht so kompliziert denken müssen, um in die Aufgabe reinzukommen, weil sie halt zeitlich Constraint sind.

**B [00:24:16]** Hätte ich diesen zeitlichen Contraint nicht, würde ich durchaus Aufgaben stellen, wo halt drei, vier, fünf Klassen sind, wovon zwei Klassen vollkommen irrelevant sind. Die machen quasi einfach nur so ein Heavy Lifting. Also die sorgen einfach nur dafür, dass irgendwas funktioniert. Und die Studis müssen sich aber nur auf zwei Klassen konzentrieren und die anderen drei können sie ignorieren. Solche Aufgaben kann ich gar nicht stellen. Das gibt mir das Tooling einfach nicht her. Wenn ich jetzt so einen Git-basierten Ansatz hätte, wo ich selber Projekte ausliefern würde, die die Studenten öffnen, bearbeiten und zeitlich vielleicht flexibel in der Bearbeitung sind, dann würde ich das auch machen. Und dann könnte ich auch kompliziertere Projekte quasi abprüfen. Das ist ein wichtiger Teil. Der ist sogar in PP weniger wichtig als in AP2. Ehrlich gesagt, weil in AP2 geht es ja um Objekt-Komposition und um Objektorientierung. Das heißt, man hat viele Objekte, viele Klassen. In PP gucken wir uns sehr isoliert einzelne Themen an. Da kommt das gar nicht so häufig vor, dass das ein Problem ist. Aber in AP2 ist es ein Problem. Und sonst gibt es bestimmt irgendwas, was ich nicht fragen kann. Achso genau, da hab ich heute sogar noch drüber nachgedacht, weil ich heute einen Test fertig gemacht habe für Freitag. Ich kann keine Fragen stellen, wo die Studenten etwas selber kreieren müssen und hochladen müssen. Also heute war Thema verkettete Liste. Ich hab mir überlegt, dass die halt zu einer Methode mir quasi die Debugger Version aufzeichnen. Also wie funktioniert das quasi, wenn man es einmal aufzeichnet, also dieser Methode Ablauf, und das kann ich halt nicht abfragen. In keiner einzigen Weise kann ich das abfragen. Ich kann es im Freitext abfragen, aber das ist halt kacke, weil die Studenten wissen nicht was sie schreiben sollen und ich weiß nicht wie ich das bewerten soll. Es wäre schöner, wenn sie was zeichnen würden, weil im Praktikum, also in den Aufgaben, müssen die was zeichnen. Ich kann's aber nicht testen, weil das E-Assessment gibt mir keine Möglichkeit quasi ein Foto hochzuladen und ich kann das vor allem auch nicht automatisiert auswerten. Also selbst das müsste ich dann auch noch per Hand auswerten. Und das ist zum Beispiel eine Art von Aufgabe, wo ich nicht weiß, ob das relevant ist. Das kann ich auf jeden Fall nicht abfragen. Das geht auch in Thema Modellierungsaufgaben. Also wenn ich jetzt zum Beispiel sage, ihr habt diese Situation und ihr müsst jetzt nicht coden, aber ihr müsst mir sagen, was für Komponenten benötige ich, um das Problem zu lösen. Also ganz einfache Entwurfsmusteraufgaben oder sowas. Weil Entwurfsmuster heißt immer viel Klassen. Und das kann ich auch nicht fragen, weil das ist einfach ein zu großer Scope. Also coden kann ich das nicht lassen, weil das ist zu viel Code. Wenn ich das aber auf so einer Diagramebene machen würde, hab ich gar keine Möglichkeiten schnell mir diese Lösung zu geben.

**I [00:26:59]** Das war schon mal ganz interessant, was man da so einbauen könnte. Ja, jetzt noch. Also DiveKit soll ja genau solche Probleme auch lösen. Und du hast vorhin schon erwähnt mit DiveKit selber hast du dich jetzt noch nicht so viel beschäftigt, was dein aktueller Stand von dem Projekt? Also kennst du die Funktionalitäten?

**B [00:27:31]** Ich habe diesen Blogpost gelesen, wo es um so ein Fallbeispiel ging mit einem Domain-Modell oder irgendwie sowas. Das hab ich angefangen zu lesen. Dann hab ich den Rest nur noch überflogen. Aber ich weiß, dass es auch irgendwas mit GitHub Repos macht oder mit Bitbucket von Atlassian. Das es irgendwas mit Repos macht und irgendwas mit Automatisierung. Mehr weiß ich aber nicht.

**I [00:27:52]** Aber von dem, was du gehört has. Denkst du, das lässt sich vor allem in AP2 und PP einsetzen?

**B: [00:28:01]** Ja, in PP weiß ich es noch nicht ganz. Aber in AP2 geht das ja so ein bisschen in die Richtung, dass man Code Projekte hat, die man einchecked, wo automatisiert dann durchgejagt werden. Wenn ich das richtig verstanden habe.

**I [00:28:19]** Okay, die Frage, welche Features noch nützlich wären, das hast ja vorhin schon gut erzählt. Und da können wir jetzt, da du jetzt nicht die einzelnen Features des Tools kennst, erübrigt sich die Frage ein bisschen bezüglich auf DiveKit jetzt.

**B [00:28:39]** Also du kannst ja versuchen, da so eine Summary zu machen von dem Tool. Also eine grobe Zusammenfassung.

**I [00:28:47]** Ja, so genau bin ich da selber auch nicht drin. Das ist das Problem. Aber ja, ich weiß, dass auf jeden Fall für das mit den Domänen für Texte schon Möglichkeiten gibt und für Freitexte soweit noch nicht. Aber ja, ich weiß nicht, was genau noch für andere Features es gab, müsste ich selber nochmal nachfragen. Aber die meisten Sachen, die du vorhin erwähnt hast, sollten auf jeden Fall damit umsetzbar sein. Und wenn es halt sehr individuell wird,  da muss noch dran gearbeitet werden.

**B [00:29:33]** Also meine größten Feature-Wünsche an so ein Tool sind, falls sie noch nicht implementiert wären: Also wenn dieses Modell mir die Möglichkeit gibt, dass ich diese Projekte halt einchecken kann oder dass Studenten Projekte einchecken und halt das ausgewertet wird und wie auch immer, dass das Ganze halt automatisiert passiert, dass ich halt eine Zuordnung habe zwischen Student und Testergebnis. Was cool wäre, ist, wenn es eine eine Plagiats-Detection geben würde. Das heißt, ich kann ja z.B. für unterschiedliche Studiengänge oder für unterschiedliche Teams durchaus unterschiedliche Aufgaben rausgeben. Und wenn ich z.B. sehe, dass Studiengang A die Implementierung einer ganz anderen Aufgabe abgibt, die aus Studiengang B kommt, ist es ja relativ eindeutig. Ja weil die können gar nicht diese Aufgabe bekommen haben. Also quasi diese Plausibilitätschecks, dass die wirklich auch ihre richtige Aufgabe abgegeben haben, das kann ja ganz über Checksumming oder sowas machen. Aber auch das wenn ich jetzt zum Beispiel drei Teams habe, die auf den Code genau, die Zeilen und auf die Zeichen genau, also auf der lexikalischen Syntax Ebene alles genau gleich haben. Ist es auch ein bisschen verdächtig. Also natürlich gibt es nur eine endliche Art und Weise, wie du etwas implementiert. Aber je nachdem, was für eine Aufgabe man hat, ist es ja doch schon unterschiedlich, vor allem wenn man Variationen einbaut. Das wäre z.B. auch gut, wenn man so eine Detection hätte.

**B [00:31:13]** Diese Detection könnte man ja durchaus frei an die Studenten kommunizieren, dass das Tool sowas kann. Das heißt, wenn sie halt Cheaten wollen, dann sollen sie sich wenigstens Mühe geben. Das wäre vielleicht auch noch ganz gut, dann finde ich den Ansatz, den der Bömer mal eine Zeitlang gemacht hat gut. Ich weiß nicht ob er das immer noch macht, aber als ich noch Betriebssysteme gemacht hab, hat er das gemacht, dass er gesagt hat, dass die Studenten Code kopieren dürfen unter sich. Die sollen das nur kennzeichnen, wenn sie das tun. Also sie sollen quasi fremden Code kennzeichnen, zitieren. Und das wird aber quasi mit einberechnet. Das heißt, du musst quasi mehr tun, wenn du nur zitierst. Also wenn du selbst auf deine Lösungen kommst, musst du nur das tun, was in der Aufgabe steht. Wenn du aber deine gesamte Aufgabe aus Zitaten hast oder ein Großteil der Aufgabe aus Zitaten hast, musst du noch eine Aufgabe machen, die du nicht zitieren darfst. Also dass es dann quasi über Quantität geht, nicht mehr über die Qualität. Das finde ich ganz gut. Das könnte man ja auch durchaus automatisiert auswerten. Und im Gegenzug wird dann aber derjenige belohnt, von dem die meisten zitieren. Das heißt, genauso wie das halt auch in so Publikationen funktioniert. Das Paper, was am meisten zitiert wird, ist am meisten geranked und das bekommt am meisten Probs. Und das könnte man ja auch irgendwie mit in die Bewertung einbauen durch Bonuspunkte, oder keine Ahnung was. Ist natürlich jetzt auch so ein bisschen angreifbar. Man kann das natürlich auch versuchen zu abusen, zu exploiten. Wenn man halt irgendwie einen außerkort, wo man sagt okay, wir haben alle die Lösungen von ihm, das heißt er wird extrem gepusht und sowas alles. Ja, kann man machen. Ich weiß nicht, ob die Studenten sich untereinander boykottieren wollen. Weiß ich nicht, was man damit auslöst. Ich finde das halt einen coolen Ansatz, den man verfolgen kann, weil diese ganze Cheat-Prevention, die kannst du halt eh nicht durchsetzen. Aber man sollte das dann wenigstens transparent machen, finde ich. Und weil dann erspart uns das halt viel Arbeit, wenn man halt von vornherein macht ihr dürft das, aber ihr müsst zitieren. Und wenn man dann aber quasi jemanden sieht und erwischt, der es nicht zitiert hat, ja dann wird es schlecht. Weil da hat man ja quasi schon vorher die Weichen gelegt. Und wenn man dann auch noch nicht darauf hört, dann ist halt schlecht. Dann ist es vielleicht ein Grund, um jemanden aus dem Praktikum auszuschließen. Das wäre gut.

**B [00:33:47]** Und was auch noch gut wäre, ist dieses Thema der Variation. Damit tue ich mich auch immer noch am meisten schwer, weil ich finde es schlecht, wenn man nur Variablennamen austauscht oder wenn man nur Klassennamen austauscht oder wenn man nur irgendetwas austauscht. Ich finde, es sollte dann auch semantisch eine gleiche Aufgabe sein, aber in einem anderen Kontext. So mache ich meine Variation. Oder ich lasse zum Beispiel unterschiedliche Methoden implementieren. Jeder bekommt eine Variation in der Implementierung der Methode. Das kann man halt nicht generieren und nicht automatisieren, also diese Art von Automatisierung. Ich glaube nicht, dass man das hinbekommt. Aber es wäre gut, wenn man z.B. irgendeine Möglichkeit hätte, da auch zu shufflen. Also man legt quasi einen Pool fest von Variationen und man bekommt ein Shuffling, was aber eine bestimmte Randomness drin hat. Also dass man z.B. sagen kann diesen Pool bekommt dieser Studiengang. Diesen Pool bekommt dieser Studiengang oder diesen Pool bekommen halt die und die Gruppen. Aber mit so einer bestimmten Randomness. Irgendwie so. Das wäre halt auch ganz nett. Ja und für mich sind eigentlich diese ganzen Coding-Sachen relevant. Also ich möchte jetzt auch nicht beide Tools benutzen, also so ein E-Assessment und dann dieses andere theoretische, DiveKit Tool oder was auch immer. Beide Tools parallel zu nutzen wäre kacke. Sehr gut, wenn es nur ein Tool wäre. Also entweder gibt halt das Tool dann auch die Features her, dass man halt eine andere Art, also dass man nicht Coding Aufgaben halt auch in dieser Art und Weise beantworten kann. Oder man benutzt beide Tools und dann hat man vielleicht eine API, eine Integration oder keine Ahnung was. Oder man muss irgendwelche Excel-Listen führen, wo man halt Ergebnisse zusammenführt.

**I [00:35:39]** Also eine Schnittstelle zu dem generellen eLearning Assessment wäre sinnvoll?

**B [00:35:45]** Eine Schnittstelle von dem Tool. Genau so, dass die Tools miteinander reden. Oder dass man ein neues Tool hat, wenn man das Microservices-mäßig aufteilt, dass man ein anderes Tool hat, was die Auswertung macht und was die Ergebnisse hat. Und das Ding spricht dann mit beiden Tools und aggregiert die Studentenergebnisse zusammen. Das kann man ja auch machen. Dass du quasi einen Teil im E-Assessement machst, ein anderes Tool machst du im DiveKit und dann anderes Tool was beide Tools aggregiert und quasi dort die kanonische Antwort auf ein Testergebnis hat, wäre auch eine Möglichkeit, das so zu machen.

**I [00:36:19]** Ja, bei DiveKit soll ja teilweise automatisierte Auswertung geben und wie hoch wäre denn da die Fehlertoleranz? Also bei automatisierten Tests kann es ja immer zu Fehlern kommen. Wie gravierend wären jetzt oder wie hoch ist die Toleranz da gegenüber, wenn die Aufgaben ja auch in Praktika oder im Zweifel in Klausuren gestellt werden?

**B [00:36:48]** Das ist eine gute Frage. Also meine naive Softwareentwickler Antwort ist, dass die Tests nicht gut genug sind, wenn es dazu Fehlertoleranz kommen würde. Wenn man halt anfängt mit Seiteneffektem, all so ein Kram, dann ist es natürlich kacke, aber wenn man halt irgendwie sich Mühe gibt in der Tests-Suite, kann ich mir erstmal nicht vorstellen, dass es da zu Problemen kommen würde. Ist aber nur theoretisch. Also theoretisch wird es auf jeden Fall zu Fehlern kommen und diese Frage ist auch sinnvoll nach Fehlertoleranz. Ich weiß erstens nicht wie man diese Fehlertoleranzen misst, weil eine Fehlertoleranz ist dann ja quasi du hast irgendein Ergebnis, ein Ergebnis des Tests und da kommt eine Studi und sagt "Ja, aber das war doch eigentlich so und so" und dann guckst du nach und sagst "Stimmt, dass es ihnen wie zufällig rot geworden" oder sowas.

**I [00:37:44]** Also ab welcher Rate von Studenten würdest zu sagen "das ist mir zuviel, da mache ich es doch lieber selber"?

**B [00:37:54]** Ich überlege, also eine Rate weiß ich nicht, ob ich das sagen kann. Aber ich meine, jeder Student hat ja eh die Möglichkeit erstmal einen Einwand vorzubringen. Sei es bei der Klausur oder sei es beim Praktikum. Und dann sagen die "Hä, ich bin mir sehr sicher, dass ich alles richtig gemacht habe. Kann ich bitte nochmal die Ergebnisse sehen?" Und dann würde man das ja finden und korrigieren. Wir haben witzigerweise jetzt gerade in dieser Testphase so einen Fall gehabt, wo sich bis jetzt bei mir so 10 Leute gemeldet haben, die zu wenig Punkte bekommen haben. Das ist sogar richtig. Das haben sie aber bekommen, weil einfach teilweise nur Fehler gemacht wurden in der Korrektur. Mir wäre das schon bei 10 Leuten, bei 10 Prozent zu viel. Ich würde sagen 95 Prozent oder irgendwie sowas. Aber ich glaube, dass es statistisch ganz sinnvoll oder 90 bis 95 oder irgendwie sowas. Das ist glaub ich überschaubar. Weil wir haben bis jetzt immer so 400 Leute, die einen Test machen. Das sind dann ja 20 bis 40 Leute. Wobei das ist noch weniger bei uns gerade. Bei uns sind das eher 10 Leute. Also ja okay, dann ist es doch so 98, 97 Prozent. Also bis 95.

**I [00:39:17]** Glaubst du wenn man so ein Tool einsetzen würde, da gäbs groß Widerspruch? Einerseits erstmal von den Studenten her oder auch vielleicht von den Professoren, für die du arbeitest?

**B [00:39:30]** Ja, beides. Ich glaube die technische Hürde ist halt viel höher, sowohl seitens der Studenten als auch seitens der Mitarbeiter. Das heißt, sie müssen sich ein neues System aneignen. Wenn man das auf Get-Basis macht, müssen die Studenten Git lernen. Git es ein ganz eigenes Ökosystem von Problemen. Stichwort Conflict-Resolving und sowas alles. Sehr schwierig. Vorteil ist aber, das ist ja Industriestandard. Die kommen halt eh früher oder später nicht daran vorbei. Außer die sagen aktiv ich habe eh keinen Bock auf Softwareentwicklung, aber dann müssen sie das halt mal eben lernen. Deshalb finde ich das gar nicht so schlecht, wenn man das quasi schon in den frühen Semestern lernt. Also dieses Argument zählt für mich nicht. Also ich sehe das Argument, aber ich finde da kann man gegen argumentieren. Und zwar sinvoll gegen argumentieren gegen diese technische Barriere seitens der Studenten. Ich glaube was die Studenten vermissen werden, ist das Feedback. Wie gesagt, ich hab ja auch gerade selber ganz am Anfang geäußert, dass ich sehe, dass dieser Feedback-Kanal zu den Studenten fehlt, wenn man alles automatisiert. Aber wenn man die Möglichkeit hat halt schlechte Leistungen zu detektieren und zu picken und dann quasi die Leute zu beraten daraufhin und zu sagen "Hey, pass auf, das kannst du sowieso besser machen". Das würde das ein bisschen auffedern. Und wenn man trotzdem den Leuten die Möglichkeit gibt, quasi aktiv Feedback einzufordern und die Leute geben denen auch Feedback. Na da ist ja auch so eine andere Sache. Man kann ja auch sagen, "ja ne, ich habe keinen Bock dem jetzt Feedback zu geben. Er hat ja seine Leistung gemacht und das ist ja nur meine Aufgabe." Das finde ich schwierig. Weil so ein technisches Tool macht halt diese Barriere weg. Man kann aber dagegen arbeiten. Deswegen ist das für mich auch kein starkes Argument.

**B [00:41:04]** So seitens der Professoren und Mitarbeitern. Das ist für mich ein starkes Argument, weil Leute sehr bequem sind und nicht unbedingt wollen, dass sich Dinge ändern in der Art und Weise, wie sie etwas machen. Ich bin großer Fan davon, das Praktikum jedes Jahr neu zu machen, weil ich gerne andere Modelle ausprobiere, was aber immer mehr Arbeit bedeutet. Aber ich seh halt, dass es teilweise den Studenten entgegenkommt. Und da ich halt selber gar nicht mal so weit entfernt von einem Studenten noch bin oder wie auch immer man das auf Deutsch sagt, ich war noch vor kurzem selber Student, sagen wir das mal so, finde ich, kann man diese Arbeit machen. Das kommt sehr stark auf den Modulvorgesetzten an. Ich glaube, dass so ein Tool akzeptiert wird, weil es nämlich einen Haufen Probleme löst, die wir haben. Es bringt aber auch neue Probleme rein, weil das natürlich nicht alles kann. Das heißt, man hat im schlimmsten Fall doppelten Verwaltungsaufwand. Also du hast quasi eine Front, die du halt über deine alten Möglichkeiten abdeckt. Und du hast den Teil, den du quasi über Automation abdeckst. Und alles was in der Automation fehlschlägt, musst du quasi mit personellen Aufwand auffedern. Also Fault Tolerance und sowas alles.

**B [00:42:21]** Fraglich, ob man quasi dieses Ding eingehen würde. Da kann ich mir vorstellen, dass das auf Widerstand stößt. Auf Widerstand stoßen heißt aber auch einfach nur die Leute machen es nicht. Also das Angebot ist da, aber es nutzt einfach nicht jeder. Das ist ja auch schon mal ein Anfang. Ich kann mir auch vorstellen, dass dadurch relativ viel Druck entstehen wird. Also wenn die Studenten z.B. sagen "Hey, Modul A, B und C machen das alle über das Tool, das ist total geil, warum macht das Modul C nicht?", dass über die Studenten Druck aufgebaut wird und das dem einen oder anderen nicht gefällt, der halt Lehrender ist. Kann ich mir auch gut vorstellen. Aber so wie ich das mitbekommen habe. Also ich weiß, dass dieses Tool wird glaub ich in Softwaretechnik oder sowas schon eingesetzt. Und den Studenten gefällt das so wie ich das mitbekommen habe. Und ich würde das halt auch anwenden bei mir in meinen Modulen. Mindestens um das mal auszuprobieren. Aber ich sehe das sogar tatsächlich auch als sinnvoll an. Aber ich weiß, dass das auf jeden Fall mehr Verwaltungsaufwand für mich bedeutet. Also selbst in dem Wissen würde ich das tun. Und ich glaube, das werden die wenigsten tun. Also das wird auf sehr viel Widerstand stoßen. Ich glaube nicht, dass das die Leute bereit sind, eine Disruption in ihrem Praktika zu erleben. Dafür braucht es personellen Wechsel. In den meisten Fällen.

**I [00:43:45]** Denkst du es könnte auch noch Widerspruch geben aufgrund von Rechtssicherheiten, wenn das Tool eingesetzt wird für Klausuren und Praktika?

**B [00:43:58]** Theoretisch ja, praktisch glaub ich nicht. Also für Klausuren da ist es glaub ich eine Katastrophe. Da kannst du es glaub ich nicht einsetzen, weil für Prüfungen muss da glaub ich rechtlich viel passieren, dass so ein Tool genehmigt wird. Muss sehr viel passieren. Deswegen da kann ich keine Antwort zu geben.

**B [00:44:15]** Zu Praktika: Theoretisch ja auf jeden Fall. Selbst uns passieren Fehler, die halt aber einfach unterlaufen, weil es einfach so viele Studenten sind, die das Tool benutzen. Also selbst mir sind schon viele Fehler passiert, die ich, wenn ich sie mitbekommen hab, korrigiert habe. Beispiel: Punkte wurden nicht richtig vergeben und sowas alles. Was aber bei so face to face Abnahmen nicht unbedingt passiert, weil da bist du ja akut in der Situation, wo du sagst bestanden oder nicht bestanden. D.h. Theoretisch besteht auf jeden Fall dort ein Problem, was schlecht für die Studenten ist. Praktisch aber können sie halt wie gesagt jederzeit Einspruch einlegen, einlegen und sagen "Hey, also ich kann mir nicht vorstellen, dass ich nur 50 Prozent in dem Test habe. Kann ich bitte die Ergebnisse sehen?" Bedeutet wieder mehr Arbeit für den Dozierenden, weil er muss die Ergebnisse bereitstellen. Und es bringt ja nichts, wenn ich dir deine Ergebnisse zeige. Ich muss dir eigentlich sagen, warum du wieviel Punkte für was bekommen hast. Das ist eigentlich ja das, worum es geht. Das heißt, es ist immer mit Arbeit verbunden und ich weiß nicht, ob da eine rechtliche Basis entsteht für die Studenten, als ob die quasi rechtlich irgendwelche Ansprüche haben oder sowas. Aber wenn man jetzt nicht unbedingt mies gestimmt gegenüber den Studenten ist, macht man das eh. Aber wie gesagt, für Prüfungen schließe ich das sogar fast aus. Für Praktika kann ich mir gut vorstellen, dass das zu Problemen führen kann. Ist aber halt in niedrigen Prozentbereichen, denke ich mal. Also wahrscheinlich auch so in fünf bis zehn Prozent Bereichen. Und das wird ja auch immer besser. Also wenn so ein Tool ernsthaft weiterentwickelt wird und wenn da ernsthaft Interesse daran besteht, das Ding weiterzuentwickeln, dann wird es ja in dieser Hinsicht auch immer besser. Sollte es zumindest sein.

**I [00:46:06]** Also zum jetzigen Zeitpunkt momentan wird es auf jeden Fall weiterentwickelt. Es wird natürlich in der Zukunft zeigen, ob das dann noch weiterentwickelt wird. Aber ja, sollte es eigentlich. Du hast ja vorhin auch erzählt, dass du selber schon mal ein bisschen rumprobiert hast mit automatisierten Tests und so weiter. Warst du vorher schon an so einem Tool wie DiveKit interessiert? Also dass du Aufgaben gestellt hast, wo du gedacht hast, wenn ich jetzt ein Tool hätte, dann wäre toll das es das macht. Oder ist das Interesse erst gekommen, als du jetzt von DiveKit gehört hast?

**B [00:46:39]** Nee, das Interesse habe ich schon seit 5 Jahren. Ich weiß sogar, dass es Hochschulen auch schon machen. Also in den Niederlanden gibt es, ich weiß nicht, ob das Utrecht ist oder sowas, da gibt es eine Uni, die macht es auf jeden Fall. Die machen alles auf Git-Basis und die machen alles mit irgendeinem CI. Ich glaube die machen das mit Jenkins oder mit Travis. Mit einem von den beiden machen die das und die machen das sogar noch mit einem Tool was sie selber geschrieben haben, was Plagiatserkennung macht. Also die haben wirklich ihre eigene Pipeline gebaut. Dieses Tool was sie benutzen, das ist sogar Open Source. Ich hab aber den Namen vergessen. Also dieses Plagiats Tool ist Open Source. Das ist aber sehr speziell für deren Anwendungsfall gecodet. Also die haben da quasi ihren Anwendungsfall hart codiert. Also das kann man sehr schlecht für andere Tools verwenden. Ich hab mir das angeguckt und seit da hab ich mir immer gedacht "Mensch, es wär doch cool, wenn man das Ganze automatisieren könnte". Aber dadurch, dass wir quasi nie diesen Bedarf hatten, sowas zu automatisieren, weil es halt die ganze Zeit Corona und Digital Kram nicht gab und die Studentenzahlen eigentlich erst in den letzten paar Jahren so nach oben geschossen sind, war der Bedarf eigentlich nie wirklich da. Also es war immer so ein schöne theoretisches Modell, aber es war nie ein akutes Modell. Und jetzt gerade durch die ganze Digitalisierung ist es ein akutes Problem. Also anders. Bei vielen ist es ein akutes Problem, die es aber trotzdem überspielen mit ihren alten Lösungen zum negativem Empfinden aller. Bei den einen funktioniert es gut bei den anderen funktioniert es schlechter. Man kann das ja auch so halb automatisiert machen, indem man einfach trotzdem diese ganzen vor Ort Abnahmen macht, aber trotzdem quasi immer ein Projekt Template zur Verfügung stellt, wo Unit Test drinsind und man gegen ein Interface implementiert. Ich meine, das ist ja so eine semi automatisierte Lösung. Das geht ja auch. Das hat früher der Erich Ehses gemacht zu der Zeit, wo ich noch AP2 gemacht habe. Jedes Praktikum war mit einem IntelliJ Projekt oder mit einem Eclipse Projekt zu dem Zeitpunkt noch, wo wirklich eine Test Suite schon drin war und man konnte das Praktikum erst abgeben, wenn mindestens alle Lampen grün waren. Man konnte es vorher gar nicht erst machen, d.h. das war ja quasi schon so halb automatisiert. Aber man musste trotzdem vor Ort abnehmen, weil man erklären musste, was man getan hat. Das finde ich sogar, ist eine sehr solide Lösung, wenn wir quasi wieder vor Ort Dinge machen können.

**B [00:49:17]** Das finde ich eigentlich ganz in Ordnung. Und von diesem Standpunkt aus, also das ist das, wie ich es erfahren habe. Der Weg zu wir benutzen jetzt einfach mal GitHub und die ganze CI Technologie, die uns zur Verfügung steht, einfach auch für Abnahmen ist relativ kurz. Also der Weg ist relativ naheliegend. Aber wie gesagt, ich hab nur einmal kurz damit herumgespielt. Aber jetzt auch für die eigentliche Frage war ja für die Module, die ich jetzt gerade akut habe. Naja, es funktioniert ja so, wie ich es mache, aber ich bin nicht zufrieden, wie es gerade ist. Also man ist natürlich nie zufrieden, aber ich bin jetzt halt nicht zufrieden und ich weiß, dass ein Teil der Lösung darin steckt. Und das ist aber fernab von dem DiveKit. Ich wusste von dem Philipp Schmeier, dass das Ding halt irgendwie existiert und im Einsatz ist. Da dachte ich mir "Cool, sowas hatte ich ja auch mal vorgehabt" und seitdem wusste ich, dass es existiert, seit zwei oder einem Jahr. Das ist ja schon ein bisschen älter, also gar nicht so neu. Ein Jahr mindestens ist es alt. Genau, aber aktiv habe ich mir das jetzt nicht gewünscht, aber ich weiß, dass es auf jeden Fall eine sinnvolle Ergänzung wäre.

**I [00:50:29]** Du hast ja generell schon viele Vorschläge und Wünsche geäußert, was du dir generell an dem  Tool wünschen würdest. Gibt es noch irgendwas, was dir noch in den Sinn kommt was du noch nicht erwähnen konntest?

**B [00:50:45]** Also als Entwickler hätte ich gerne APIs dazu. Zumindest lesende APIs auf die ganzen Ergebnisse. Also das es quasi so Plugin-System mäßig wäre, dass man selber eigene Automatisierungen reinknüppeln kann. Also so modular quasi, dass nicht alle Last auf dem Entwicklerteam hängen bleibt, sondern dass man sagen kann wie bei CI, also wie bei so einer richtigen Continuous Integration Pipeline oder Continuous Delivery Pipeline, wo man sagen kann "okay, ich bau mir jetzt wirklich meine eigene Action", die ich dann da reinknüppel. Ja, dass es ist wirklich so eine Pipeline ist und ich kann selber meine eigenen Actions einbauen. Das wäre geil. In welcher Sprache und Technologie und in welchem System auch immer das funktioniert ist erstmal egal. Das wäre ein cooles Feature. Einfach nur aus Entwicklersicht, weil dann könnte ich nämlich das ein bisschen customizen an meine Bedürfnisse, weil ich sehr stark davon ausgehe, dass eh nicht alle Bedürfnisse halt irgendwo unterkommen können. Dann einfach nur APIs um halt selber irgendwelche Tools vielleicht zu schreiben, die mir dabei helfen, sas mit dem E-Assessment zu kombinieren. Also dass ich mindestens lesende Zugriffe hab, wenn nicht, dass ich sogar ein API Token bekomme, womit ich sogar auch schreibender Zugriff auf irgendwelche Ergebnisse haben kann oder sowas. Oder das es halt in irgendeiner Art und Weise als Microservice oder als Self-Contained System implementiert ist, dass es irgendwie einen Messenger Bus gibt, also Richtung Kafka oder irgendwie sowas, wo man halt Ergebnisse pushen kann oder Ergebnisse subscriben kann. Also wenn das jetzt nicht unbedingt eine Rest API ist oder sowas. Also irgendeine Art sich an das System zu hooken wäre gut. Das ist erstmal so aus Developersicht. Was ich halt auch als Gefahr sehe, was aber eine Gefahr eines jeden Tools ist, dass wenn da die Entwickler weg sind, dass das Tool dann einfach nicht mehr weiterentwickelt wird und da einfach eine Leiche ist, obwohl Leute darauf aufbauen. Das ist ja immer ein Problem. Das heißt eine Anforderung wäre, dass es halt Open Source entwickelt wird. Also entweder öffentlich Open Source oder Hochschulintern Open Source. Und zwar am liebsten auch mit einem Stack, der nicht esoterisch ist, mit dem die Leute weiterentwickeln können und Forks bauen können, falls die ursprünglichen Leute sich davon verabschieden. Kann ja mal passieren. Das heißt das sollte jetzt nicht irgendwie ein Haskell Backend sein mit einem Pure Script Frontend was drei Menschen auf dieser Welt können, sondern das es halt irgendwie so Technologie verwendet wo sich mehr Leute halt das ganze anschaffen könnten. Also das sind alles nicht funktionale Anforderungen. Ansonsten von den funktionalen Anforderungen hab ich glaub ich was mir spontan einfällt alles gesagt. Ja, ich glaube sonst hab ich alles.

**I [00:53:37]** Na gut, ich glaube das waren die Fragen auch soweit. Kleine Frage an die Protokollantin: Ist da noch was aufgefallen? Irgendetwas, was noch fehlt?

**P [00:53:44]** Ich war fleißig am Mitschreiben. Aber ich glaube du hast alles gestellt.

**I [00:53:50]** Perfekt. Dann wären wir soweit durch mit dem Interview. Vielen Dank, dass du dir Zeit genommen hast.

**B [00:53:57]** Ja, kein Problem.

**I [00:53:59]** Und ja, ich denke, wir können die Aufnahmen jetzt beenden.

**P [00:54:09]** Auch von mir herzlichen Dank an dieser Stelle.

